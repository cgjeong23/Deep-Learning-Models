{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1GN1vCMs052qK0mFaPbf2a7D6QoQrIwZH",
      "authorship_tag": "ABX9TyNgnGhshTzVdHcFddAGwy2i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cgjeong23/Deep-Learning-Models/blob/main/RNN_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJWbSkgIw4AG",
        "outputId": "e65c3568-97ce-4bf3-c268-2f15df34dc43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader"
      ],
      "metadata": {
        "id": "u1nnGMC2sApG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "import torch\n",
        "\n",
        "class SequenceDataset(Dataset):\n",
        "\n",
        "  def __init__(self, sequence, tokenizer_file='monkeypox_tokenizer.json'):\n",
        "    \"\"\"sequence: List of str\n",
        "    \n",
        "    [\"ACTG......\", \"GTCA.....]\"\"\"\n",
        "    self.sequence = sequence\n",
        "\n",
        "    self.tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "    self.tokenizer.pre_tokenizer = Whitespace()\n",
        "    self.tokenizer = self.tokenizer.from_file(tokenizer_file)\n",
        "    self.tokenizer.enable_padding()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sequence)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    seq = self.sequence[idx]\n",
        "    encoded_seq = self.tokenizer.encode(seq)\n",
        "    return torch.LongTensor(encoded_seq.ids)"
      ],
      "metadata": {
        "id": "Nab-2rHcsJRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "ciQdG4bDsKCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = SequenceDataset(['ACTGACTACTGACGATCGACTGG','ACTGACCACTGACTGATCGGTG'])\n",
        "train_loader = DataLoader(dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "mwf3_HadsLxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SOmyebkuuSt",
        "outputId": "3b99ec53-345c-47d0-fc94-110b059b92bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2716, 1470,   69,   64,   15])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, pad_id, hidden_dim, num_layers):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, paddin_idx=pad_id)\n",
        "    self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "    self.out_layer = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  def forward(self, ids):\n",
        "    # ids: [batch size, max sequence length] as [B, L]\n",
        "    embedded_ids = self.embedding(ids) # [B, L, E]\n",
        "    rnn_out, _ = self.rnn(embedded_ids) # [B, L, H]\n",
        "    return self.out_layer(rnn_out) # [B, L, V]"
      ],
      "metadata": {
        "id": "Rv-BEpaWuwq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ],
      "metadata": {
        "id": "CykLI01csMgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Rb2CzQLYsNbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "-A91Bd_AsN5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, dataloader, loss_function, lr, epoch):\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  history = []\n",
        "  for e in range(epoch):\n",
        "\n",
        "    pbar = tqdm(dataloader)  \n",
        "\n",
        "    average_loss = []\n",
        "    for batch_sequence in pbar:\n",
        "\n",
        "      batch_sequence = batch_sequence.to('cuda')\n",
        "      x = batch_sequence[:, :-1]\n",
        "      y = batch_sequence[:, 1:] # [B, L]\n",
        "      h = model(x) # [B, L, V]\n",
        "      h = h.permute(0, 2, 1) # [B, V, L]\n",
        "      j = loss_function(h,y)\n",
        "\n",
        "      # do gread descent\n",
        "      optimizer.zero_grad()\n",
        "      j.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      average_loss.append(j.item())\n",
        "\n",
        "    history.append(np.mean(average_loss))\n",
        "\n",
        "    \n",
        "  return history\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "CVS1YcjjsPgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "tTw0Vt71sPzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(model, testX, testy, device):\n",
        "  testX = torch.FloatTensor(testX).to(device)\n",
        "  out = model(testX)\n",
        "  pred = out.argmax(-1) # shape of (10000,)\n",
        "  pred = pred.cpu().numpy()\n",
        "  \n",
        "  acc = (pred == testy).sum() / pred.shape[0] #array of (10000) boolean\n",
        "\n",
        "  return acc\n",
        "\n"
      ],
      "metadata": {
        "id": "jIIfdDIT17xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Do Training"
      ],
      "metadata": {
        "id": "G4KJNdkj4YA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = ['ACTG','GCTA']\n",
        "tokenizer_file = 'monkeypox_tokenizer.json'"
      ],
      "metadata": {
        "id": "gXJAVMMr5ySF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-4\n",
        "batch_size =\n",
        "num_epochs = 5\n",
        "vocab_size = dataset.tokenizer.get_vocab_size()\n",
        "pad_id = dataset.tokenizer.padding['pad_id']\n",
        "embedding_dim = 256\n",
        "hidden_dim = 512\n",
        "num_layers = 1"
      ],
      "metadata": {
        "id": "n6jXeaKF5ghS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNModel(vocab_size, embedding_dim, pad_id, hidden_dim, num_layers)\n",
        "dataset = SequenceDataset(sequence, tokenizer_file)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=pad_id)\n"
      ],
      "metadata": {
        "id": "jNcax7aE4eFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_history = train(model, dataloader, loss_function, lr, epoch)"
      ],
      "metadata": {
        "id": "hWvp2vpp4YMf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}